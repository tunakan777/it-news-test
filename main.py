# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import requests
from bs4 import BeautifulSoup
import time
from file_handler import save_to_json
from analyzer import analyze_from_json

# ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã®URL
URL = "https://b.hatena.ne.jp/hotentry/it"

# è¨ªå•ã®é€£çµ¡å‘ªæ–‡
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

print("=" * 60)
print("ğŸš€ ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼†åˆ†æãƒ„ãƒ¼ãƒ«")
print("=" * 60)

print("\nğŸ“¡ ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
response = requests.get(URL, headers=headers)

# ã‚¿ã‚°ã‚’è¦‹ã‚„ã™ã„å½¢ã«
soup = BeautifulSoup(response.content, "html.parser")

# è¨˜äº‹ï¼‘å€‹ãšã¤ã®ç®±ã‚’ã€å…¨éƒ¨è¦‹ã¤ã‘å‡ºã™
entries = soup.find_all("li", class_="cat-it")

# ãœã‚“ã¶ã®æƒ…å ±ã‚’è²¯ã‚ã¦ãŠããŸã‚ã®ã€ç©ºã£ã½ã®ãƒªã‚¹ãƒˆã‚’ç”¨æ„
ranking_data = [] 

print(f"âœ… {len(entries)}ä»¶ã®è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸã€‚å‡¦ç†é–‹å§‹ï¼\n")
print("-" * 60)

# forãƒ«ãƒ¼ãƒ—ã§ã€ç®±ã‚’ä¸€å€‹ãšã¤è¦‹ã¦ã„ã
for idx, entry in enumerate(entries, 1):
    
    # h3ã‚¿ã‚°ã‚’æ¢ã™
    h3_tag = entry.find("h3", class_="entrylist-contents-title")
    
    # ã‚‚ã—ã€h3ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã£ãŸã‚‰â€¦
    if h3_tag:
        # h3ã‚¿ã‚°ã®ä¸­ã‹ã‚‰ã€aã‚¿ã‚°ã‚’æ¢ã™ï¼
        a_tag = h3_tag.find("a")
        
        # ã‚‚ã—ã€aã‚¿ã‚°ãŒè¦‹ã¤ã‹ã£ãŸã‚‰â€¦
        if a_tag:
            # <a>ã‚¿ã‚°ã‹ã‚‰ã€ã€Œtitleï¼ˆè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã€ã‚’å¼•ã£ã“æŠœã
            title = a_tag.get("title")
            
            # <a>ã‚¿ã‚°ã‹ã‚‰ã€ã€Œhrefï¼ˆè¨˜äº‹ã®ãƒªãƒ³ã‚¯ï¼‰ã€ã‚’å¼•ã£ã“æŠœã
            link = a_tag.get("href")
            
            # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ã‚’æŠœã„ã¦ãã‚‹
            users_span = entry.find("span", class_="entrylist-contents-users")
            
            # ã‚¨ãƒ©ãƒ¼ã‚’èµ·ã“ã•ãªã„ã‚ˆã†ã«
            if users_span:
                users_text = users_span.find("a").find("span").text
                hatebu_count = int(users_text.replace(" users", ""))
            else:
                hatebu_count = 0
            
            # æŠ•ç¨¿æ—¥æ™‚ã‚’æŠœã„ã¦ãã‚‹
            data_li = entry.find("li", class_="entrylist-contents-date")
            
            if data_li:
                published_date = data_li.text.strip()
            else:
                published_date = ""
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸ã«ã¾ã¨ã‚ã‚‹
            article_data = {
                "title": title,
                "link": link,
                "hatebu_count": hatebu_count,
                "published_date": published_date
            }
            
            # ğŸ”¥ ã“ã“ãŒé‡è¦ï¼ ranking_dataã«è¿½åŠ ã™ã‚‹
            ranking_data.append(article_data)
            
            # ç”»é¢è¡¨ç¤º
            print(f"[{idx}] {title}")
            print(f"    ğŸ“Œ {hatebu_count} users | ğŸ•’ {published_date}")
            print(f"    ğŸ”— {link}")
            print("-" * 60)

# æœ€çµ‚ãƒã‚§ãƒƒã‚¯ï¼
print("\n" + "=" * 60)
print("ğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã‚µãƒãƒªãƒ¼")
print("=" * 60)
print(f"âœ… å–å¾—ã—ãŸè¨˜äº‹æ•°: {len(ranking_data)}ä»¶")
print(f"ğŸ“ JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ä¸­...")

# JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
save_to_json(ranking_data)

print("=" * 60)
print("\nğŸ”¥ æ¬¡ã¯æ€¥ä¸Šæ˜‡ãƒ¯ãƒ¼ãƒ‰åˆ†æã‚’é–‹å§‹ã—ã¾ã™ï¼\n")

# ğŸ‰ ã“ã“ã§æ€¥ä¸Šæ˜‡ãƒ¯ãƒ¼ãƒ‰åˆ†æã‚’å®Ÿè¡Œï¼
trending_words = analyze_from_json("hatena_ranking.json", top_n=15)

print("\n" + "=" * 60)
print("ğŸŠ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
print("=" * 60)
print("ğŸ“„ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
print("  - hatena_ranking.json (è¨˜äº‹ãƒ‡ãƒ¼ã‚¿)")
print("  - trending_words.json (æ€¥ä¸Šæ˜‡ãƒ¯ãƒ¼ãƒ‰)")
print("=" * 60)