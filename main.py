# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import requests
from bs4 import BeautifulSoup
import time
from file_handler import save_to_json

# ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã®URL
URL = "https://b.hatena.ne.jp/hotentry/it"

# è¨ªå•ã®é€£çµ¡å‘ªæ–‡
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

print("ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
response = requests.get(URL, headers=headers)

# ã‚¿ã‚°ã‚’è¦‹ã‚„ã™ã„å½¢ã«
soup = BeautifulSoup(response.content, "html.parser")

# è¨˜äº‹ï¼‘å€‹ãšã¤ã®ç®±ã‚’ã€å…¨éƒ¨è¦‹ã¤ã‘å‡ºã™
entries = soup.find_all("li", class_="cat-it")

# ãœã‚“ã¶ã®æƒ…å ±ã‚’è²¯ã‚ã¦ãŠããŸã‚ã®ã€ç©ºã£ã½ã®ãƒªã‚¹ãƒˆã‚’ç”¨æ„
ranking_data = [] 

print(f"\n{len(entries)}ä»¶ã®è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸã€‚å‡¦ç†é–‹å§‹ï¼\n")

# forãƒ«ãƒ¼ãƒ—ã§ã€ç®±ã‚’ä¸€å€‹ãšã¤è¦‹ã¦ã„ã
for idx, entry in enumerate(entries, 1):
    
    # h3ã‚¿ã‚°ã‚’æ¢ã™
    h3_tag = entry.find("h3", class_="entrylist-contents-title")
    
    # ã‚‚ã—ã€h3ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã£ãŸã‚‰â€¦
    if h3_tag:
        # h3ã‚¿ã‚°ã®ä¸­ã‹ã‚‰ã€aã‚¿ã‚°ã‚’æ¢ã™ï¼
        a_tag = h3_tag.find("a")
        
        # ã‚‚ã—ã€aã‚¿ã‚°ãŒè¦‹ã¤ã‹ã£ãŸã‚‰â€¦
        if a_tag:
            # <a>ã‚¿ã‚°ã‹ã‚‰ã€ã€Œtitleï¼ˆè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã€ã‚’å¼•ã£ã“æŠœã
            title = a_tag.get("title")
            
            # <a>ã‚¿ã‚°ã‹ã‚‰ã€ã€Œhrefï¼ˆè¨˜äº‹ã®ãƒªãƒ³ã‚¯ï¼‰ã€ã‚’å¼•ã£ã“æŠœã
            link = a_tag.get("href")
            
            # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ã‚’æŠœã„ã¦ãã‚‹
            users_span = entry.find("span", class_="entrylist-contents-users")
            
            # ã‚¨ãƒ©ãƒ¼ã‚’èµ·ã“ã•ãªã„ã‚ˆã†ã«
            if users_span:
                users_text = users_span.find("a").find("span").text
                hatebu_count = int(users_text.replace(" users", ""))
            else:
                hatebu_count = 0
            
            # æŠ•ç¨¿æ—¥æ™‚ã‚’æŠœã„ã¦ãã‚‹
            data_li = entry.find("li", class_="entrylist-contents-date")
            
            if data_li:
                published_date = data_li.text.strip()
            else:
                published_date = ""
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸ã«ã¾ã¨ã‚ã‚‹
            article_data = {
                "title": title,
                "link": link,
                "hatebu_count": hatebu_count,
                "published_date": published_date
            }
            
            # ğŸ”¥ ã“ã“ãŒé‡è¦ï¼ ranking_dataã«è¿½åŠ ã™ã‚‹
            ranking_data.append(article_data)
            
            # ç”»é¢è¡¨ç¤º
            print(f"[{idx}] ã‚¿ã‚¤ãƒˆãƒ«: {title}")
            print(f"    ãƒªãƒ³ã‚¯: {link}")
            print(f"    ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°: {hatebu_count}")
            print(f"    å…¬é–‹æ—¥: {published_date}")
            print("--------------------")

# æœ€çµ‚ãƒã‚§ãƒƒã‚¯ï¼
print("\n--- ğŸ”¥ğŸ”¥ğŸ”¥æœ€çµ‚ãƒã‚§ãƒƒã‚¯ï¼ranking_dataã®ä¸­èº«ã¯ã“ã‚Œã ï¼ğŸ”¥ğŸ”¥ğŸ”¥ ---")
print(f"å–å¾—ã—ãŸè¨˜äº‹æ•°: {len(ranking_data)}ä»¶")
print(ranking_data)

# æœ€å¾Œã«ã€ä¸€å›ã ã‘ã€ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã®ãƒ—ãƒ­ã‚’å‘¼ã³å‡ºã™ï¼
print("\nJSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ä¸­...")
save_to_json(ranking_data)
print("âœ… å®Œäº†ï¼")